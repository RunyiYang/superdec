{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d90e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"Extract a single object point cloud from a ScanNet scene.\n",
    "\n",
    "The script reads the down-sampled ScanNet point cloud with RGB + nyu40 labels\n",
    "(`*_vh_clean_2.labels.ply`), the corresponding segmentation indices\n",
    "(`*_vh_clean_2.0.010000.segs.json`), and the aggregation metadata\n",
    "(`*_vh_clean.aggregation.json`). A target object can be specified either by the\n",
    "majority nyu40 class id, the textual label in the aggregation file, or the\n",
    "object id reported there. The resulting subset of points is written to an ASCII\n",
    "PLY file that preserves xyz, rgb(a), and label information so it can be easily\n",
    "visualised or reused for custom segmentation experiments.\n",
    "\"\"\"\n",
    "import argparse\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def find_single_file(scene_dir: Path, pattern: str) -> Path:\n",
    "    matches = sorted(scene_dir.glob(pattern))\n",
    "    if not matches:\n",
    "        raise FileNotFoundError(f\"No files matching {pattern!r} in {scene_dir}\")\n",
    "    if len(matches) > 1:\n",
    "        raise RuntimeError(\n",
    "            f\"Expected one file matching {pattern!r} in {scene_dir}, found {len(matches)}\"\n",
    "        )\n",
    "    return matches[0]\n",
    "\n",
    "\n",
    "def load_vertices(ply_path: Path) -> np.ndarray:\n",
    "    with ply_path.open(\"rb\") as f:\n",
    "        header: List[str] = []\n",
    "        while True:\n",
    "            line = f.readline().decode(\"ascii\").strip()\n",
    "            header.append(line)\n",
    "            if line == \"end_header\":\n",
    "                header_len = f.tell()\n",
    "                break\n",
    "        vertex_count: Optional[int] = None\n",
    "        for line in header:\n",
    "            if line.startswith(\"element vertex\"):\n",
    "                vertex_count = int(line.split()[2])\n",
    "                break\n",
    "        if vertex_count is None:\n",
    "            raise RuntimeError(f\"Could not find vertex count in {ply_path}\")\n",
    "        dtype = np.dtype(\n",
    "            [\n",
    "                (\"x\", \"<f4\"),\n",
    "                (\"y\", \"<f4\"),\n",
    "                (\"z\", \"<f4\"),\n",
    "                (\"red\", \"u1\"),\n",
    "                (\"green\", \"u1\"),\n",
    "                (\"blue\", \"u1\"),\n",
    "                (\"alpha\", \"u1\"),\n",
    "                (\"label\", \"<u2\"),\n",
    "            ]\n",
    "        )\n",
    "        f.seek(header_len)\n",
    "        vertices = np.fromfile(f, dtype=dtype, count=vertex_count)\n",
    "    return vertices\n",
    "\n",
    "\n",
    "def build_segment_index(seg_indices: Iterable[int]) -> dict[int, np.ndarray]:\n",
    "    segment_to_indices: defaultdict[int, List[int]] = defaultdict(list)\n",
    "    for idx, seg_id in enumerate(seg_indices):\n",
    "        segment_to_indices[int(seg_id)].append(idx)\n",
    "    return {seg_id: np.array(indices, dtype=np.int64) for seg_id, indices in segment_to_indices.items()}\n",
    "\n",
    "\n",
    "def gather_indices(group_segments: Iterable[int], segment_to_indices: dict[int, np.ndarray]) -> np.ndarray:\n",
    "    indices: List[np.ndarray] = []\n",
    "    for seg_id in group_segments:\n",
    "        if seg_id in segment_to_indices:\n",
    "            indices.append(segment_to_indices[seg_id])\n",
    "    if not indices:\n",
    "        return np.array([], dtype=np.int64)\n",
    "    return np.concatenate(indices)\n",
    "\n",
    "\n",
    "def write_ascii_ply(path: Path, vertices: np.ndarray) -> None:\n",
    "    with path.open(\"w\", encoding=\"ascii\") as f:\n",
    "        f.write(\"ply\\n\")\n",
    "        f.write(\"format ascii 1.0\\n\")\n",
    "        f.write(f\"element vertex {len(vertices)}\\n\")\n",
    "        f.write(\"property float x\\n\")\n",
    "        f.write(\"property float y\\n\")\n",
    "        f.write(\"property float z\\n\")\n",
    "        f.write(\"property uchar red\\n\")\n",
    "        f.write(\"property uchar green\\n\")\n",
    "        f.write(\"property uchar blue\\n\")\n",
    "        f.write(\"property uchar alpha\\n\")\n",
    "        f.write(\"property ushort label\\n\")\n",
    "        f.write(\"end_header\\n\")\n",
    "        for v in vertices:\n",
    "            f.write(\n",
    "                f\"{v['x']:.6f} {v['y']:.6f} {v['z']:.6f} \"\n",
    "                f\"{int(v['red'])} {int(v['green'])} {int(v['blue'])} \"\n",
    "                f\"{int(v['alpha'])} {int(v['label'])}\\n\"\n",
    "            )\n",
    "\n",
    "\n",
    "def choose_group(groups: List[dict], target_nyu: Optional[int], target_label: Optional[str], target_object_id: Optional[int], segment_to_indices: dict[int, np.ndarray], vertex_labels: np.ndarray) -> dict:\n",
    "    candidates: List[dict] = []\n",
    "    for group in groups:\n",
    "        indices = gather_indices(group[\"segments\"], segment_to_indices)\n",
    "        group[\"_vertex_indices\"] = indices\n",
    "        if indices.size == 0:\n",
    "            group[\"_nyu40\"] = None\n",
    "            continue\n",
    "        counts = Counter(vertex_labels[indices].tolist())\n",
    "        group[\"_nyu40\"] = counts.most_common(1)[0][0]\n",
    "        if target_nyu is not None and group[\"_nyu40\"] == target_nyu:\n",
    "            candidates.append(group)\n",
    "        elif target_label is not None and group[\"label\"] == target_label:\n",
    "            candidates.append(group)\n",
    "        elif target_object_id is not None and group[\"objectId\"] == target_object_id:\n",
    "            candidates.append(group)\n",
    "    if target_nyu is None and target_label is None and target_object_id is None:\n",
    "        raise ValueError(\"No selection criteria provided\")\n",
    "    if not candidates:\n",
    "        raise RuntimeError(\"No aggregation group matched the provided criteria\")\n",
    "    if len(candidates) > 1:\n",
    "        labels = [(g[\"objectId\"], g[\"label\"], g.get(\"_nyu40\")) for g in candidates]\n",
    "        raise RuntimeError(f\"Selection is ambiguous; matched groups: {labels}\")\n",
    "    return candidates[0]\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    parser = argparse.ArgumentParser(description=__doc__)\n",
    "    parser.add_argument(\"--scene-dir\", type=Path, required=True, help=\"Path to the ScanNet scene folder (e.g. scene0000_00)\")\n",
    "    group = parser.add_mutually_exclusive_group(required=True)\n",
    "    group.add_argument(\"--nyu40-id\", type=int, help=\"nyu40 class id to extract (e.g. 33 for toilet)\")\n",
    "    group.add_argument(\"--object-label\", type=str, help=\"Object label string in the aggregation file\")\n",
    "    group.add_argument(\"--object-id\", type=int, help=\"Object id from the aggregation file\")\n",
    "    parser.add_argument(\"--output\", type=Path, required=True, help=\"Destination PLY file path\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    scene_dir: Path = args.scene_dir\n",
    "    ply_path = find_single_file(scene_dir, \"*_vh_clean_2.labels.ply\")\n",
    "    segs_path = find_single_file(scene_dir, \"*_vh_clean_2.0.010000.segs.json\")\n",
    "    agg_path = find_single_file(scene_dir, \"*_vh_clean.aggregation.json\")\n",
    "\n",
    "    vertices = load_vertices(ply_path)\n",
    "    with segs_path.open() as f:\n",
    "        seg_indices = np.array(json.load(f)[\"segIndices\"], dtype=np.int64)\n",
    "    with agg_path.open() as f:\n",
    "        agg = json.load(f)\n",
    "    groups = agg[\"segGroups\"]\n",
    "\n",
    "    if seg_indices.shape[0] != vertices.shape[0]:\n",
    "        raise RuntimeError(\n",
    "            \"Mismatch between vertex count in PLY and segmentation indices: \"\n",
    "            f\"{vertices.shape[0]} vs {seg_indices.shape[0]}\"\n",
    "        )\n",
    "\n",
    "    segment_to_indices = build_segment_index(seg_indices)\n",
    "    target_group = choose_group(\n",
    "        groups,\n",
    "        target_nyu=args.nyu40_id,\n",
    "        target_label=args.object_label,\n",
    "        target_object_id=args.object_id,\n",
    "        segment_to_indices=segment_to_indices,\n",
    "        vertex_labels=vertices[\"label\"],\n",
    "    )\n",
    "\n",
    "    indices = target_group.get(\"_vertex_indices\", np.array([], dtype=np.int64))\n",
    "    if indices.size == 0:\n",
    "        raise RuntimeError(\"Selected group contains no vertices after segmentation lookup\")\n",
    "    subset = vertices[indices]\n",
    "\n",
    "    args.output.parent.mkdir(parents=True, exist_ok=True)\n",
    "    write_ascii_ply(args.output, subset)\n",
    "\n",
    "    print(\n",
    "        f\"Wrote {len(subset)} vertices for objectId {target_group['objectId']} \"\n",
    "        f\"('{target_group['label']}'), nyu40={target_group.get('_nyu40')} to {args.output}\"\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
